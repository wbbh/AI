{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "\"\"\"\nQ-Learning example using OpenAI gym MountainCar enviornment\nAuthor: Moustafa Alzantot (malzantot@ucla.edu)\n\"\"\"\n#请补全???的部分\n#附加题：用policy iteration或value iteration解决这个例子\n\nimport numpy as np\n\nimport gym\nfrom gym import wrappers\n\nn_states \u003d 40\niter_max \u003d 10000\n\ninitial_lr \u003d 1.0 # Learning rate\nmin_lr \u003d 0.003\ngamma \u003d 1.0\nt_max \u003d 10000\neps \u003d 0.02\n\ndef run_episode(env, policy\u003dNone, render\u003dFalse):\n    obs \u003d env.reset()\n    total_reward \u003d 0\n    step_idx \u003d 0\n    for _ in range(t_max):\n        if render:\n            env.render()\n        if policy is None:\n            action \u003d env.action_space.sample()\n        else:\n            a,b \u003dobs_to_state(env,obs)          #得到离散化的状态\n            action \u003dpolicy[a][b]          #按照policy决定下次action\n        obs, reward, done, _ \u003d env.step(action)\n        total_reward +\u003d gamma ** step_idx * reward\n        step_idx +\u003d 1\n        if done:\n            break\n    return total_reward\n\ndef obs_to_state(env, obs):             #该函数的作用的是将连续的状态转换成有限的离散状态\n    \"\"\" Maps an observation to state \"\"\"\n    env_low \u003d env.observation_space.low\n    env_high \u003d env.observation_space.high\n    env_dx \u003d (env_high - env_low) / n_states\n    a \u003d int((obs[0] - env_low[0])/env_dx[0])\n    b \u003d int((obs[1] - env_low[1])/env_dx[1])\n    return a, b\n\nif __name__ \u003d\u003d \u0027__main__\u0027:\n    env_name \u003d \u0027MountainCar-v0\u0027\n    env \u003d gym.make(env_name)\n    env.seed(0)\n    np.random.seed(0)\n    print (\u0027----- using Q Learning -----\u0027)\n    q_table \u003dnp.zeros(n_states,n_states,3)  #初始化q表\n    for i in range(iter_max):\n        obs \u003d env.reset()\n        total_reward \u003d 0\n        ## eta: learning rate is decreased at each step\n        eta \u003d max(min_lr, initial_lr * (0.85 ** (i//100)))\n        for j in range(t_max):\n            a, b \u003d obs_to_state(env, obs)\n            if np.random.uniform(0, 1) \u003c eps:\n                action \u003d np.random.choice(env.action_space.n)\n            else:\n                logits \u003d q_table[a][b]\n                logits_exp \u003d np.exp(logits)\n                probs \u003d logits_exp / np.sum(logits_exp)\n                action\u003dnp.random.choice(env.action_space.n,p\u003dprobs)\n                #按照概率probs选择下一个action\n            obs, reward, done, _ \u003d env.step(action)\n            total_reward +\u003d (gamma ** j) * reward\n            # update q table\n            a_, b_ \u003d obs_to_state(env, obs)\n            q_table[a][b][action]\u003dq_table[a][b][action]+eta*(reward+gamma*np.max(q_table[a_][b_][action])-q_table[a][b][action])   #更新q表\n            if done:\n                break\n        if i % 100 \u003d\u003d 0:\n            print(\u0027Iteration #%d -- Total reward \u003d %d.\u0027 %(i+1, total_reward))\n    solution_policy \u003d np.argmax(q_table, axis\u003d2)\n    solution_policy_scores \u003d [run_episode(env, solution_policy, False) for _ in range(100)]\n    print(\"Average score of solution \u003d \", np.mean(solution_policy_scores))\n    # Animate it\n    run_episode(env, solution_policy, True)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}